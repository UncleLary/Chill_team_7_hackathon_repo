import os
from pathlib import Path
from typing import Dict, Any, Optional

# AutoGen v0.4 imports
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient

MAX_RETRIES = 5
PROMPT_TEMPLATE_PATH = os.path.join(Path(__file__).parent.parent.absolute(), "prompt_templates")
API_KEY = os.environ['OPENAI_API_KEY']
MODEL_NAME = os.environ['OPENAI_MODEL_NAME']

class AgentWorkflowEngine:
    """
    Orchestrates a two-agent flow (Processor and Verifier) using Microsoft AutoGen v0.4.
    """

    def __init__(self):
        self.template_dir = Path(PROMPT_TEMPLATE_PATH)

        self.model_name = MODEL_NAME
        self.api_key = API_KEY

        # Create the model client reused by agents
        self.model_client = OpenAIChatCompletionClient(
            model=self.model_name,
            api_key=self.api_key,
        )

    def _load_and_render_template(self, template_name: str, agent_suffix: str, context: Dict[str, Any]) -> str:
        """
        Loads a template file and renders placeholders with provided context.
        Files are expected to be named: <template_name>.<agent_suffix>
        """
        filename = f"{template_name}.{agent_suffix}"
        file_path = self.template_dir / filename

        if not file_path.exists():
            raise FileNotFoundError(f"Template file not found: {file_path}")

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
            
            # Use safe format to inject context variables into the prompt
            return content.format(**context)
        except KeyError as e:
            raise ValueError(f"Missing context argument for template {filename}: {e}")
        except Exception as e:
            raise RuntimeError(f"Error rendering template {filename}: {e}")

    async def run_flow(self, template_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """
        Executes the Processor -> Verifier loop using AutoGen v0.4 agents.
        """
        print(f"Running LLM workflow with template: {template_name} and arguments: {arguments} Key: {self.api_key} Model: {self.model_name}")
        # 1. Load and Render System Prompts
        processor_system_msg = self._load_and_render_template(template_name, "agent1", arguments)
        verifier_system_msg = self._load_and_render_template(template_name, "agent2", arguments)

        # 2. Initialize Agents (v0.4 Style)
        # In v0.4, we inject the model_client directly.
        processor = AssistantAgent(
            name="Processor",
            system_message=processor_system_msg,
            model_client=self.model_client,
        )

        verifier = AssistantAgent(
            name="Verifier",
            system_message=verifier_system_msg + "\n\nCRITICAL: You must end your response with exactly 'VERIFIED: TRUE' or 'VERIFIED: FALSE'.",
            model_client=self.model_client,
        )

        final_output = ""
        verification_feedback = ""
        is_verified = False
        attempts = 0

        # 3. Execution Loop
        for i in range(MAX_RETRIES):
            attempts += 1
            
            # --- Step A: Processor Generates ---
            if i == 0:
                processor_prompt = "Please perform the task defined in your system instructions."
            else:
                processor_prompt = f"Your previous attempt was rejected. \nVerifier Feedback: {verification_feedback}\n\nPlease try again, fixing these issues."

            # v0.4: Call on_messages directly. No UserProxy needed for programmatic loops.
            # We wrap the string prompt in a TextMessage with source="user".
            proc_response = await processor.on_messages(
                messages=[TextMessage(content=processor_prompt, source="user")],
                cancellation_token=None
            )
            
            # Extract content from the Response object
            current_output = proc_response.chat_message.content
            
            # --- Step B: Verifier Checks ---
            verify_prompt = (
                f"Please review the following output generated by the Processor:\n"
                f"--- BEGIN OUTPUT ---\n{current_output}\n--- END OUTPUT ---\n\n"
                f"Is this satisfactory based on your criteria? Provide feedback and end with VERIFIED: TRUE/FALSE."
            )

            # Note: We create a fresh conversation context for the verifier each time
            # by passing only the current prompt. The agent logic is stateless here
            # unless we explicitly maintain a list of previous messages.
            ver_response = await verifier.on_messages(
                messages=[TextMessage(content=verify_prompt, source="user")],
                cancellation_token=None
            )

            verifier_text = ver_response.chat_message.content

            # --- Step C: Parse Boolean Result ---
            if "VERIFIED: TRUE" in verifier_text.upper():
                final_output = current_output
                is_verified = True
                verification_feedback = verifier_text
                break
            else:
                verification_feedback = verifier_text
                # Loop continues for retry

        return {
            "template_used": template_name,
            "attempts_made": attempts,
            "success": is_verified,
            "final_processor_output": final_output if is_verified else current_output,
            "final_verifier_feedback": verification_feedback
        }
    
async def get_agent_workflow_engine():
    yield AgentWorkflowEngine()
